{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jieba工具库使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 小米/在/网易/云/课堂/学习/自然/自然语言/语言/处理\n",
      "Default Mode: 小米/在/网易/云/课堂/学习/自然语言/处理\n",
      "小米，在，网易，云，课堂，学习，自然，语言，自然语言，处理\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"小米在网易云课堂学习自然语言处理\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/\".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"小米在网易云课堂学习自然语言处理\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/\".join(seg_list))  # 精确模式（默认模式）\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小米在网易云课堂学习自然语言处理\")   # 搜索引擎模式\n",
    "print(\"，\".join(seg_list))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加用户自定义字典\n",
    "* 1.jieba.load_userdict(file_name)  \n",
    "* 2.少量词汇可自己手动添加  \n",
    "    * add_word(word, freq=None, tag=None)和del_word(word)在程序中动态修改字典  \n",
    "    * suggest_freq(segment, tune=True)可调节单个词语的词频，使其能（或不能）被分出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/字典/中将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut(\"如果放到字典中将出错。\", HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/字典/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "jieba.suggest_freq(('中', '将'), tune=True)\n",
    "print('/'.join(jieba.cut(\"如果放到字典中将出错。\", HMM=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我  r\n",
      "在  p\n",
      "网易  n\n",
      "云  ns\n",
      "学习  v\n",
      "自然语言  l\n",
      "处理  v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "words = pseg.cut(\"我在网易云学习自然语言处理\")\n",
    "for word, flag in words:\n",
    "    print(\"%s  %s\" % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词抽取\n",
    "### 基于TF-IDF算法的关键词抽取\n",
    "* import jieba.analyse\n",
    "* jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "\n",
    "算法细节   \n",
    "TF(词频)，IDF(逆文档频率)  \n",
    "**第一步，计算词频** \n",
    " $$词频（TF）= 某个词在文章中出现的次数$$\n",
    " 考虑到文章有长短之分，为了便于不同文章的比较，进行词频标准化。\n",
    " $$词频（TF）= \\frac{某个词在文章中出现的次数}{文章的总词数}$$\n",
    " 或者\n",
    "  $$词频（TF）= \\frac{某个词在文章中出现的次数}{该文出现次数最多的词的出现次数}$$\n",
    "  \n",
    "**第二步，计算逆文档频率**\n",
    "这时，需要一个语料库（corpus），用来模拟语言的使用环境。\n",
    "$$逆文档频率（IDF）= log(\\frac{语料库的文档总数}{包含该词的文档数+1})$$\n",
    "如果一个词越常见，那么分母就越大，逆文档频率就越接近于0。  \n",
    "\n",
    "**第三步，计算TF-IDF**\n",
    "$$TF-IDF=词频(TF)\\times逆文档频率(IDF)$$\n",
    "\n",
    "* jieba.analyse.set_idf_path(file_name)   # 自定义语料库路径\n",
    "* jieba.analyse.set_stop_words(file_name) # 自定义停用词语料库路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韦少 杜兰特 全明星 全明星赛 MVP 威少 正赛 科尔 投篮 勇士 球员 斯布鲁克 更衣柜 NBA 三连庄 张卫平 西部 指导 雷霆 明星队\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "\n",
    "lines = open('../Data/NBA.txt', encoding='utf-8').read()\n",
    "print(\" \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者 八戒 师父 三藏 唐僧 大圣 沙僧 妖精 菩萨 和尚 那怪 那里 长老 呆子 徒弟 怎么 不知 老孙 国王 一个\n"
     ]
    }
   ],
   "source": [
    "lines = open('../Data/西游记.txt', encoding='utf-8').read()\n",
    "print(\" \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TextRank算法的关键词抽取\n",
    "* jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn'))\n",
    "* jieba.analyse.TextRank()  新建自定义TextRank实例\n",
    "\n",
    "基本思想：  \n",
    "* 将待抽取关键词的文本进行分词；\n",
    "* 以固定的窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图；\n",
    "* 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者 师父 八戒 三藏 大圣 不知 菩萨 妖精 只见 长老 国王 却说 呆子 徒弟 小妖 出来 不得 不见 不能 师徒\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
