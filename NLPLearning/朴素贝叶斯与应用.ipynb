{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯理论\n",
    "在我们有一堆样本（包含特征和类别）的时候，我们非常容易通过统计得到P(特征|类别)  \n",
    "其公式如下：$$ P(x)P(y|x) = P(y)P(x|y) $$  \n",
    "做下变换：  \n",
    "$$ P(特征)P(类别|特征) = P(类别)P(特征|类别)$$  \n",
    "$$ P(类别|特征)=\\frac{P(类别)P(特征|类别)}{P(特征)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 独立假设\n",
    "实际上，特征可能有很多维  \n",
    "$$P(features|class) = P(f_0,f_1,...,f_n|c) $$  \n",
    "若假设两个维度，可以简单写成：  \n",
    "$$ P(f_0,f_1|c)=P(f_1|c,f_0)P(f_0|c) $$  \n",
    "这时候我们加一个很牛逼的假设：特征之间的独立的，这样就得到：  \n",
    "$$ P(f_0,f_1|c)=P(f_1|c)P(f_0|c) $$  \n",
    "其实也就是：\n",
    "$$ P(f_0,f_1,...,f_n|c)=\\prod_i^nP(f_i|c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯分类器\n",
    "回到机器学习，其实我们就是对每个类别计算一个概率P(c_i)，然后在计算所有特征的条件概率P(f_i|c_i)，那么分类的时候我们就是依据贝叶斯找一个最可能的类别：  \n",
    "$$ P(class_i|f_0,f_1,...,f_n)=\\frac{P(class_i)}{P(f_0,f_1,...,f_n)}\\prod_j^nP(f_i|c_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类问题\n",
    "下面我们来看一个文本分类问题，经典的新闻主题分类，用朴素贝叶斯怎么做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba    # 处理中文\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 粗暴的词去重\n",
    "def make_word_set(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip().decode('utf-8')\n",
    "            if len(word)>0 and word not in words_set:  # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本处理，也就是样本生成过程\n",
    "def text_processing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "    \n",
    "    # 遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path, folder)\n",
    "        files = os.listdir(new_folder_path)\n",
    "        # 读取文件\n",
    "        j = 1\n",
    "        for file in files:\n",
    "            if j>100:  # 怕内存爆掉，只取100个样本\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path,file), 'r') as fp:\n",
    "                raw = fp.read()\n",
    "                \n",
    "            # 使用jieba中文分词\n",
    "            #jieba.enable_parallel(4)    # 开启并行分词模式，参数为并行进程数，不支持windows\n",
    "            word_cut = jieba.cut(raw, cut_all=False) # 精确输出，返回的结构是一个可迭代的genertor\n",
    "            word_list =  list(word_cut)  # genertor转化为list，每个词unicode模式\n",
    "            #jieba.disable_parallel()  # 关闭并行分词模式\n",
    "            \n",
    "            data_list.append(word_list)  # 训练集list\n",
    "            class_list.append(folder.decode('utf-8'))  # 类别\n",
    "            j += 1\n",
    "            \n",
    "        # 粗暴地划分训练集和测试集\n",
    "        data_class_list = zip(data_list, class_list)\n",
    "        random.shuffle(data_class_list)\n",
    "        index = int(len(data_class_list)*test_size) + 1\n",
    "        train_list = data_class_list[index:]\n",
    "        test_list = data_class_list[:index]\n",
    "        train_data_list, train_class_list = zip(*train_list)\n",
    "        test_data_list, test_class_list = zip(*test_list)\n",
    "        \n",
    "        # 统计词频放入all_words_dict\n",
    "        all_words_dict = {}\n",
    "        for word_list in train_data_list:\n",
    "            for word in word_list:\n",
    "                if all_words_dict.has_key(word):\n",
    "                    all_words_dict[word] += 1\n",
    "                else:\n",
    "                    all_words_dict[word] = 1\n",
    "                    \n",
    "        # key函数利用词频进行降序排列，内建函数sorted参数需为list\n",
    "        all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True)\n",
    "        all_words_dict = list(zip(*all_words_tuple_list)[0])\n",
    "        \n",
    "        return all_words_dict, train_data_list, test_data_list, train_class_list, test_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    # 选取词特征\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000:  # features_words的维度为1000\n",
    "            break\n",
    "            \n",
    "        if not all_words_list[t].isdigit() \\\n",
    "        and all_words_list[t] not in stopwords_set and 1<len(all_words_list[t]<5):\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n += 1\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本特征\n",
    "def text_featrues(train_data_list, test_data_list, feature_words, flag='nltk'):\n",
    "    def text_featrues(text, feature_words):\n",
    "        text_words = set(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
