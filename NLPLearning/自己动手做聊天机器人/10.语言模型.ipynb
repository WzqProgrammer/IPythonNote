{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数学模型\n",
    "数学模型是运用数理逻辑方法和数学语言建构的科学或工程模型。说白了，就是用数学的方式来解释事实。举个简单的例子：你有一只铅笔，又捡了一只，一共是两只，数学模型就是1+1=2。举个复杂的例子：你在路上每周能捡到3只铅笔，数学模型就是P(X)=3/7，这个数学模型可以帮你预测明天捡到铅笔的可能性。当然解释实事的数学模型不是唯一的，比如每周捡三只铅笔的数学模型还可能是P(qt=sj|qt-1=si,qt-2=sk,...)，s=0,1，也就是有两个状态的马尔可夫模型，意思就是明天是否捡到铅笔取决于前几天有没有捡到铅笔  \n",
    "<br/> \n",
    "### 1.1 数学建模\n",
    "数学建模就是通过计算得到的结果来解释实际问题，并接受实际的检验，来建立数学模型的全过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 语言模型\n",
    "语言模型是根据语言客观事实而进行的语言抽象数学建模。说白了，就是找到一个数学模型，让它来解释自然语言的事实。  \n",
    "<br/> \n",
    "### 2.1 n元语法模型\n",
    "业界目前比较认可而且有效的语言模型是**n元语法模型(n-gram model)**，它本质上是马尔可夫模型，简单来描述就是：**一句话中下一个词的出现和最近n个词有关(包括它自身)**。详细解释一下：  \n",
    "如果这里的n=1时，那么最新一个词只和它自己有关，也就是它是独立的，和前面的词没关系，这叫做一元文法  \n",
    "如果这里的n=2时，那么最新一个词和它前面一个词有关，比如前面的词是“我”，那么最新的这个词是“是”的概率比较高，这叫做二元文法，也叫作一阶马尔科夫链  \n",
    "依次类推，工程上n=3用的是最多的，因为n越大约束信息越多，n越小可靠性更高  \n",
    "n元语法模型实际上是一个概率模型，也就是出现一个词的概率是多少，或者一个句子长这个样子的概率是多少。  \n",
    "这就又回到了之前文章里提到的自然语言处理研究的两大方向：基于规则、基于统计。n元语法模型显然是基于统计的方向。  \n",
    "<br/>  \n",
    "#### 2.1.1 N-最短路径分词法\n",
    "在前n-1个词出现的条件下，下一个词出现的概率是有统计规律的，这个规律为中文自动分词提供了统计学基础，所以出现了这么几种统计分词方法：N-最短路径分词法、基于n元语法模型的分词法  \n",
    "\n",
    "N-最短路径分词法其实就是一元语法模型，每个词成为一元，独立存在，出现的概率可以基于大量语料统计得出，比如“确实”这个词出现概率的0.001（当然这是假设，别当真），我们把一句话基于词表的各种切词结果都列出来，因为字字组合可能有很多种，所以有多个候选结果，这时我们利用每个词出现的概率相乘起来，得到的最终结果，谁最大谁就最有可能是正确的，这就是N-最短路径分词法。  \n",
    "\n",
    "这里的N的意思是说我们计算概率的时候最多只考虑前N个词，因为一个句子可能很长很长，词离得远，相关性就没有那么强了  \n",
    "\n",
    "这里的最短路径其实是传统最短路径的一种延伸，由加权延伸到了概率乘积  \n",
    "\n",
    "而基于n元语法模型的分词法就是在N-最短路径分词法基础上把一元模型扩展成n元模型，也就是统计出的概率不再是一个词的概率，而是基于前面n个词的条件概率  \n",
    "<br/>  \n",
    "### 2.2 概率是如何统计的\n",
    "说到基于统计，那么就要说概率是如何估计的了，通常都是使用**最大似然估计**，怎么样理解“最大似然估计”，最大似然就是最最最最最相似的，那么和谁相似，和历史相似，历史是什么样的？10个词里出现过2次，所以是2/10=1/5，所以经常听说过的“最大似然估计”就是用历史出现的频率来估计概率的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 语言模型的困难  \n",
    "<br/> \n",
    "### 3.1 千变万化的自然语言导致的0概率问题\n",
    "基于统计的自然语言处理需要基于大量语料库进行，而自然语言千变万化，可以理解所有词汇的笛卡尔积，数量大到无法想象，有限的语料库是难以穷举语言现象的，因此n元语法模型会出现某一句话出现的概率为0的情况，比如我这篇博客在我写出来之前概率就是0，因为我是原创。那么这个0概率的问题如何解决呢？这就是业界不断在研究的数据平滑技术，也就是通过各种数学方式来让每一句话的概率都大于0。具体方法不列举，都是玩数学的，比较简单，无非就是加个数或者减个数或者做个插值平滑一下，效果上应用在不同特点的数据上各有千秋。平滑的方法确实有效，各种自然语言工具中都实现了，直接用就好了。  \n",
    "<br/>  \n",
    "### 3.2 特定领域的特定词概率偏大问题\n",
    "每一种领域都会有一些词汇比正常概率偏大，比如计算机领域会经常出现“性能”、“程序”等词汇，这个解决办法可以通过缓存一些刚刚出现过的词汇来提高后面出现的概率来解决。当然这里面是有很多技巧的，我们并不是认为所有出现过的词后面概率都较大，而是会考虑这些词出现的频率和规律(如：词距)来预测。  \n",
    "<br/>  \n",
    "### 3.3 单一语言模型总会有弊端\n",
    "还是因为语料库的不足，我们会融合多种语料库，但因为不同语料库之间的差异，导致我们用单一语言模型往往不够准确，因此，有一种方法可以缓和这种不准确性，那就是把多种语言模型混到一起来计算，这其实是一种折中，这种方法low且有效。  \n",
    "<br/> \n",
    "还有一种方法就是用多种语言模型来分别计算，最后选择熵最大的一种，这其实也是一种折中，用在哪种地方就让哪种模型生效。  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 神经网络语言模型\n",
    "21世纪以来，统计学习领域无论什么都要和深度学习搭个边，毕竟计算机计算能力提升了很多，无论多深都不怕。神经网络语言模型可以看做是一种特殊的模型平滑方式，本质上还是在计算概率，只不过通过深层的学习来得到更正确的概率。  \n",
    "<br/>  \n",
    "## 5. 语言模型的应用\n",
    "这几乎就是自然语言处理的应用了，有：中文分词、机器翻译、拼写纠错、语音识别、音子转换、自动文摘、问答系统、OCR等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 中文分词  \n",
    "<br/> \n",
    "### 6.1 历史进程\n",
    "话说上个世纪，中文自动分词还处于初级阶段，每句话都要到汉语词表中查找，有没有这个词？有没有这个词？所以研究集中在：怎么查找最快、最全、最准、最狠......，所以就出现了正向最大匹配法、逆向最大匹配法、双向扫描法、助词遍历法......，用新世纪比较流行的一个词来形容就是：你太low了！  \n",
    "\n",
    "中文自动分词最难的两个问题：1）歧义消除；2）未登陆词识别。  \n",
    "说句公道话，没有上个世纪那么low的奠定基础，也就没有这个世纪研究重点提升到这两个高级的问题（ps:未登录词就是新词，词表里没有的词）\n",
    "\n",
    "本世纪计算机软硬件发展迅猛，计算量存储量都不再是问题，因此基于统计学习的自动分词技术成为主流，所以就出现了各种新分词方法，也更适用于新世纪文本特点。  \n",
    "<br/> \n",
    "### 6.2 由字构词\n",
    "由字构词的分词方法出现可以说是一项突破，发明者也因此得到了各项第一和很多奖项，那么这个著名的分词法是怎么做的呢？  \n",
    "\n",
    "每个字在词语中都有一个构词位置：词首、词中、词尾、单独构词。根据一个字属于不同的构词位置，我们设计出来一系列特征，比如：前一个词、前两个词、前面词长度、前面词词首、前面词词尾、前面词词尾加上当前的字组成的词……  \n",
    "\n",
    "我们基于大量语料库，利用平均感知机分类器对上面特征做打分，并训练权重系数，这样得出的模型就可以用来分词了，句子右边多出来一个字，用模型计算这些特征的加权得分，得分最高的就是正确的分词方法。  \n",
    "\n",
    "分词方法很多，效果上一定是有区别的，基于n元语法模型的方法的优势在于词表里已有的词的分词效果，基于字构词的方法的优势在于未登陆词的识别，因此各有千秋，你适合哪个就用哪个。  \n",
    "\n",
    "既然两种分词各有优缺点，那么就把他们结合起来吧，来个插值法折中一下，用过的人都说好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 流行的分词工具\n",
    "### 7.1 jieba中文分词\n",
    "官方描述：  \n",
    "* 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)\n",
    "* 采用了动态规划查找最大概率路径, 找出基于词频的最大切分 组合\n",
    "* 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法  \n",
    "\n",
    "前两句话是说它是基于词表的分词，最后一句是说它也用了由字构词，所以它结合了两种分词方法  \n",
    "<br/> \n",
    "\n",
    "### 7.2 ik分词器\n",
    "基于词表的最短路径切词  \n",
    "<br/> \n",
    "### 7.3 LTP 云平台分词\n",
    "主要基于机器学习框架并部分结合词表的方法  \n",
    "<br/> \n",
    "其他分词工具判断方法类似，网上对各种分词工具好坏的判断多数是功能上比较，个人建议通过原理来判断，如果结合了基于词表和由字构词并且充分利用统计学习的方法，这样的分词工具才是最好的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
