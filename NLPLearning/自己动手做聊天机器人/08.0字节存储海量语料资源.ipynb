{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 关键词提取\n",
    "互联网资源无穷无尽，如何获取到我们所需的那部分语料库呢？这需要我们给出特定的关键词，而基于问句的关键词提取上一节已经做了介绍，利用pynlpir库可以非常方便地实现关键词提取，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电脑 \t 2.0\n",
      "垃圾 \t 2.0\n",
      "文件 \t 2.0\n",
      "删除 \t 1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pynlpir\n",
    "\n",
    "pynlpir.open()\n",
    "s = '怎么才能把电脑里的垃圾文件删除'\n",
    "\n",
    "key_words = pynlpir.get_key_words(s, weighted=True)\n",
    "for key_word in key_words:\n",
    "    print(key_word[0],'\\t',key_word[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 充分利用搜索引擎\n",
    "有了关键词，想获取预料信息，还需要知道几大搜索引擎的调用接口，首先我们来探索一下百度，百度的接口是这样的：  \n",
    "https://www.baidu.com/s?wd=机器学习 数据挖掘 信息检索  \n",
    "把wd参数换成我们的关键词就可以拿到相应的结果，我们用程序来尝试一下：  \n",
    "首先创建scrapy工程，执行：  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=yellow>scrapy startproject baidu_search</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动生成了baidu_search目录和下面的文件:  \n",
    "<font color=yellow>scrapy genspider baidu_search1 baidu_search1.com</font>  \n",
    "<br/> \n",
    "创建baidu_search/baidu_search/spiders/baidu_search.py文件，内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import sys\n",
    "\n",
    "class BaiduSearch1Spider(scrapy.Spider):\n",
    "    name = 'baidu_search1'\n",
    "    allowed_domains = ['baidu.com']\n",
    "    start_urls = ['http://www.baidu.com/s?wd=机器学习']\n",
    "\n",
    "    def parse(self, response):\n",
    "        print(response.body)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这样我们的抓取器就做好了，进入baidu_search/baidu_search/目录，执行："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=yellow>scrapy crawl baidu_search</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在settings.py中修改各项配置，如协议、用户代理、超时设置等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) \\\n",
    "            AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "\n",
    "DOWNLOAD_TIMEOUT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次执行：scrapy crawl baidu_search1  \n",
    "<br/> \n",
    "可以看到大片大片的html了，现在将其写道临时文件中，修改parse()函数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def parse(self, response):\n",
    "        filename = \"result.html\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 语料提取\n",
    "上面得到的仅是搜索结果，它只是一种索引，真正的内容需要进入到每一个链接才能拿到，下面我们尝试提取出每一个链接并继续抓取里面的内容，那么如何提取链接呢，我们来分析一下result.html这个抓取百度搜索结果文件  \n",
    "<br/> \n",
    "我们可以看到，每一条链接都是嵌在class=c-container这个div里面的一个h3下的a标签的href属性  \n",
    "修改parse()函数并添加如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = response.selector.xpath('//div[contains(@class, \"c-container\")]/h3/a/@href').extract()\n",
    "for href in hrefs:\n",
    "    print href"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "现在将这些url添加到抓取队列中继续抓取，修改baidu_search1.py文件，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def parse(self, response):\n",
    "        hrefs = response.selector.xpath('//div[contains(@class, \"c-container\")]/h3/a/@href').extract()\n",
    "        for href in hrefs:\n",
    "            yield scrapy.Request(href, callback=self.parse_url)\n",
    "\n",
    "    def parse_url(self, response):\n",
    "        print len(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正常抓取后后将抓取下来的网页提取出正文并尽量去掉标签，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\ipykernel_launcher.py:3: ScrapyDeprecationWarning: Module `scrapy.utils.markup` is deprecated. Please import from `w3lib.html` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import sys\n",
    "from scrapy.utils.markup import remove_tags\n",
    "\n",
    "class BaiduSearch1Spider(scrapy.Spider):\n",
    "    name = 'baidu_search1'\n",
    "    allowed_domains = ['baidu.com']\n",
    "    start_urls = ['http://www.baidu.com/s?wd=机器学习']\n",
    "\n",
    "    def parse(self, response):\n",
    "        hrefs = response.selector.xpath('//div[contains(@class, \"c-container\")]/h3/a/@href').extract()\n",
    "        containers = response.selector.xpath('//div[contains(@class, \"c-container\")]')\n",
    "        \n",
    "        for container in containers:\n",
    "            href = container.xpath('h3/a/@href').extract()[0]\n",
    "            title = remove_tags(container.xpath('h3/a').extract()[0])\n",
    "            c_abstract = container.xpath('div/div/div[contains(@class, \"c-abstract\")]').extract()\n",
    "            \n",
    "            abstract = \"\"\n",
    "            if len(c_abstract) > 0:\n",
    "                abstract = remove_tags(c_abstract[0])\n",
    "            \n",
    "            request = scrapy.Request(href, callback=self.parse_url)\n",
    "            request.meta['title'] = title\n",
    "            request.meta['abstract'] = abstract\n",
    "            yield request\n",
    "            \n",
    "            \n",
    "    def parse_url(self, response):\n",
    "        print(\"url:\", response.url)\n",
    "        print(\"title:\", response.meta['title'])\n",
    "        print(\"abstract:\", response.meta['abstract'])\n",
    "        content = remove_tags(response.selector.xpath('//body').extract()[0])\n",
    "        print(\"content_len\", len(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释一下，首先我们在提取url的时候顺便把标题和摘要都提取出来，然后通过scrapy.Request的meta传递到处理函数parse_url中，这样在抓取完成之后也能接到这两个值，然后提取出content，这样我们想要的数据就完整了：url、title、abstract、content  \n",
    "<br/> \n",
    "百度搜索数据几乎是整个互联网的镜像，所以你想要得到的答案，我们的语料库就是整个互联网，而我们完全借助于百度搜索引擎，不必提前存储任何资料，互联网真是伟大！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
